---
sidebar_position: 17
---

# ุจุงุจ 17: ุจุงุช ฺุช ฺฉุฑู ูุงู ุฑูุจููนฺฉุณ (Conversational Robotics)

## ุชุนุงุฑู

ุฌุฏุฏ ุงูุณุงู ููุง ุฑูุจููน (Humanoid Robots) ฺฉู ุงูุณุงููฺบ ฺฉ ุณุงุชฺพ ูุทุฑ ุงูุฏุงุฒ ูฺบ ุจุงุช ฺุช ฺฉุฑู ฺฉ ุถุฑูุฑุช   ุจุงุจ ุฑูุจููน ฺฉููนุฑูู ุณุณูนูุฒ ฺฉ ุณุงุชฺพ ูุงุฑุฌ ููฺฏูุฌ ูุงฺูุฒ (LLMs) ฺฉู ูุฑุจูุท ฺฉุฑู ฺฉุง ุงุญุงุท ฺฉุฑุชุง  ุชุงฺฉ ุขูุงุฒ ฺฉ ุฐุฑุน ุชุนุงูู ุงูุฑ ูุนูู ุงุณุชุฏูุงู (Semantic Reasoning) ููฺฉู ู ุณฺฉ

:::tip ุชุนูู ููุงุตุฏ
- ROS 2 ฺฉ ุณุงุชฺพ LLMs (ุฌุณ GPT-4, Llama 2) ฺฉุง ุงูุถูุงู
- ุงุณูพฺ ูนู ูนฺฉุณูน (STT) ุงูุฑ ูนฺฉุณูน ูนู ุงุณูพฺ (TTS) ฺฉุง ููุงุฐ
- "Voice-to-Action" ูพุงุฆูพ ูุงุฆูุฒ ฺฉ ุชุงุฑ
- ุฑูุจููนฺฉุณ ฺฉ ู ูพุฑุงููพูน ุงูุฌูุฆุฑูฺฏ (Prompt Engineering) ฺฉุง ฺุฒุงุฆู
- ูุนูู ุงุณุชุฏูุงู (Semantic Reasoning) ฺฉุง ููุงุฐ
:::

## ุขูุงุฒ ฺฉ ุชุนุงูู ฺฉ ูพุงุฆูพ ูุงุฆู (Voice Interaction Pipeline)

### 1. ุงุณูพฺ ูนู ูนฺฉุณูน (Whisper)

```python
import whisper
import pyaudio
import wave

class SpeechRecognizer:
    def __init__(self):
        self.model = whisper.load_model("base")
        self.audio_format = pyaudio.paInt16
        self.channels = 1
        self.rate = 16000
    
    def record_audio(self, duration=5):
        """Record audio from microphone"""
        p = pyaudio.PyAudio()
        stream = p.open(format=self.audio_format,
                        channels=self.channels,
                        rate=self.rate,
                        input=True,
                        frames_per_buffer=1024)
        
        print("Listening...")
        frames = []
        for _ in range(0, int(self.rate / 1024 * duration)):
            data = stream.read(1024)
            frames.append(data)
            
        print("Processing...")
        stream.stop_stream()
        stream.close()
        p.terminate()
        
        # Save temporary file
        with wave.open("temp.wav", "wb") as wf:
            wf.setnchannels(self.channels)
            wf.setsampwidth(p.get_sample_size(self.audio_format))
            wf.setframerate(self.rate)
            wf.writeframes(b''.join(frames))
            
        return "temp.wav"
    
    def transcribe(self, audio_file):
        """Convert speech to text"""
        result = self.model.transcribe(audio_file)
        return result["text"]
```

### 2. ูนฺฉุณูน ูนู ุงุณูพฺ (Coqui / EdgeTTS)

```python
import edge_tts
import asyncio

class TextToSpeech:
    def __init__(self):
        self.voice = "en-US-AriaNeural"
    
    async def speak(self, text, output_file="output.mp3"):
        """Convert text to speech"""
        communicate = edge_tts.Communicate(text, self.voice)
        await communicate.save(output_file)
        
        # Play audio
        import os
        os.system(f"mpg123 {output_file}")

# Usage wrapper
def say(text):
    tts = TextToSpeech()
    asyncio.run(tts.speak(text))
```

## LLM ุงูุถูุงู (LLM Integration)

### ุฑูุจููนุณ ฺฉ ู ูพุฑุงููพูน ุงูุฌูุฆุฑูฺฏ

```python
SYSTEM_PROMPT = """
You are a humanoid robot assistant. You can perform physical actions and answer questions.
Your capabilities:
1. pick_up(object_name)
2. move_to(location)
3. wave_hand()
4. look_at(target)

When asked to do something:
- If it's a physical action, output a JSON command: {"action": "name", "params": {...}}
- If it's a question, answer naturally.
- If it's ambiguous, ask for clarification.

User: "Pick up the red bottle."
Assistant: {"action": "pick_up", "params": {"object": "red bottle"}}

User: "Go to the kitchen."
Assistant: {"action": "move_to", "params": {"location": "kitchen"}}

User: "Hello!"
Assistant: "Hello! How can I help you today?"
"""
```

### ChatGPT / GPT-4 ุงูุถูุงู

```python
import openai
import json

class RobotBrain:
    def __init__(self, api_key):
        openai.api_key = api_key
        self.history = [{"role": "system", "content": SYSTEM_PROMPT}]
    
    def think(self, user_input):
        """Process user input and decide action"""
        self.history.append({"role": "user", "content": user_input})
        
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=self.history,
            temperature=0.7
        )
        
        reply = response.choices[0].message.content
        self.history.append({"role": "assistant", "content": reply})
        
        # Check if reply is a command
        try:
            command = json.loads(reply)
            return command, True  # Command, is_action
        except json.JSONDecodeError:
            return reply, False   # Text reply, is_action
```

## Voice-to-Action ROS 2 ููฺ

### ูฺฉูู ุงูุถูุงู (Complete Integration)

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String

class VoiceActionNode(Node):
    def __init__(self):
        super().__init__('voice_action_node')
        
        # Modules
        self.ear = SpeechRecognizer()
        self.brain = RobotBrain("YOUR_API_KEY")
        self.mouth = TextToSpeech()
        
        # Publishers
        self.cmd_pub = self.create_publisher(String, '/robot_commands', 10)
        
        # Main loop
        self.timer = self.create_timer(1.0, self.loop)
        self.is_listening = False
        
    def loop(self):
        if self.is_listening:
            return
            
        self.is_listening = True
        
        # 1. Listen
        audio_file = self.ear.record_audio()
        text = self.ear.transcribe(audio_file)
        
        if text.strip():
            self.get_logger().info(f"Heard: {text}")
            
            # 2. Think
            response, is_action = self.brain.think(text)
            
            if is_action:
                self.get_logger().info(f"Command: {response}")
                
                # Publish to robot controller
                msg = String()
                msg.data = json.dumps(response)
                self.cmd_pub.publish(msg)
                
                # Confirm action
                action_name = response['action'].replace('_', ' ')
                say(f"Okay, I will {action_name}")
            else:
                # 3. Speak
                self.get_logger().info(f"Reply: {response}")
                say(response)
        
        self.is_listening = False

def main():
    rclpy.init()
    node = VoiceActionNode()
    rclpy.spin(node)
```

## ูุนูู ุงุณุชุฏูุงู (Semantic Reasoning)

### ููุธุฑ ฺฉู ุณูุฌฺพูุง (Scene Understanding)

```python
class SceneReasoning:
    def __init__(self, visual_module, brain):
        self.eyes = visual_module
        self.brain = brain
    
    def describe_scene(self):
        """Generate description of visible objects"""
        objects = self.eyes.detect_objects()
        
        description = "I see: " + ", ".join(
            [f"{obj['class']} at {obj['pos']}" for obj in objects]
        )
        return description
    
    def handle_complex_request(self, request):
        """
        User: "Give me something to drink"
        Robot: *looks* -> sees bottle -> "pick_up bottle"
        """
        scene_desc = self.describe_scene()
        
        prompt = f"""
        Context: You are in a room. {scene_desc}
        User Request: "{request}"
        Task: Which object should I interact with to satisfy the request?
        """
        
        response = self.brain.ask(prompt)
        return response
```

## ููุงู LLMs (Llama 2 / Mistral)

ุฑุงุฒุฏุงุฑ ุงูุฑ ุขู ูุงุฆู ุตูุงุญุช ฺฉ ูุ ููุงู ูุงฺูุฒ ฺฉุง ุงุณุชุนูุงู ฺฉุฑฺบ:

```python
from langchain.llms import LlamaCpp

class LocalBrain:
    def __init__(self):
        self.llm = LlamaCpp(
            model_path="./llama-2-7b-chat.gguf",
            n_gpu_layers=30,  # GPU acceleration
            n_ctx=2048,
            temperature=0.7
        )
    
    def think(self, text):
        prompt = f"User: {text}\nAssistant:"
        return self.llm(prompt)
```

## ุจุงุช ฺุช ฺฉุฑู ูุงู ุฑูุจููนฺฉุณ ฺฉุง ูุณุชูุจู

- **ูููน ูุงฺู ูุงฺูุฒ:** GPT-4V (ูฺู + ุฒุจุงู)
- **Embodied AI:** PaLM-E, RT-2 (ุจุฑุง ุฑุงุณุช ูฺู ุณ ุงฺฉุดู ุชฺฉ)
- **ุฌุฐุจุงุช ุฐุงูุช:** ุตุงุฑู ฺฉ ุฌุฐุจุงุช ฺฉุง ูพุช ูฺฏุงูุง
- **ุฐุงุช ููุนุช:** ููุช ฺฉ ุณุงุชฺพ ุตุงุฑู ฺฉ ุชุฑุฌุญุงุช ุณฺฉฺพูุง

## ุฎูุงุต

โ ุงุณูพฺ ูนู ูนฺฉุณูน ุงูุฑ ูนฺฉุณูน ูนู ุงุณูพฺ
โ ุณูนุฑฺฉฺุฑฺ ฺฉูุงูฺุฒ ฺฉ ู ูพุฑุงููพูน ุงูุฌูุฆุฑูฺฏ
โ Voice-to-Action ROS 2 ูพุงุฆูพ ูุงุฆู
โ ูุนูู ููุธุฑ ฺฉุง ุงุณุชุฏูุงู (Semantic Scene Reasoning)
โ ููุงู LLM ุงูุถูุงู

## ูพุฑฺฉูนุณ ูุดูฺบ

1. ุงฺฉ ุจูุงุฏ "Simon Says" ุฑูุจููน ุจูุงุฆฺบ
2. ุจุตุฑ QA ูุงฺฏู ฺฉุฑฺบ (ุฑูุจููน ุฌู ุฏฺฉฺพุชุง  ุงุณ ฺฉ ุจุงุฑ ูฺบ ุณูุงูุงุช ฺฉุง ุฌูุงุจ ุฏูุง)
3. ูููน ูนุฑู ฺฉููุฑุณุดู ูฺูุฑ (Multi-turn conversation handler) ุจูุงุฆฺบ
4. Jetson Orin ูพุฑ ุงฺฉ ููุงู LLM ุชุนูุงุช ฺฉุฑฺบ

## ูุชุฌ

ูุจุงุฑฺฉ ู! ุขูพ ู **ูุฒฺฉู AI ุงูุฑ ูููุงุฆฺ ุฑูุจููนฺฉุณ** ฺฉ ูุตุงุจ ฺฉุชุงุจ ูฺฉูู ฺฉุฑ ู  ุงุจ ุขูพ ฺฉ ูพุงุณ ุฌุฏุฏ ูููุงุฆฺ ุฑูุจููนุณ ุจูุงูุ ุงู ฺฉ ููู (simulate) ฺฉุฑูุ ฺฉููนุฑูู ฺฉุฑู ุงูุฑ ุงู ุณ ุจุงุช ฺฉุฑู ฺฉุง ุนูู ููุฌูุฏ 

**ุขฺฏ ฺฉุง ุ**
- ุฑูุจููนฺฉุณ ฺฉููููน ูฺบ ุดุงูู ูฺบ
- ุงูพูุง ุงุฑฺ ูุฆุฑ ุจูุงุฆฺบ
- ุงููพู ุณูุฑุณ ูฺบ ุญุต ฺุงูฺบ
- ุชุฌุฑุจุงุช ฺฉุฑุช ุฑฺบ!

---

:::tip ฺฉูุฑุณ ูฺฉูู! ๐
ุขูพ ู ุงุฑฺ ูุฆุฑ ุณ ู ฺฉุฑ ุฌุฏุฏ AI ุชฺฉ ูพูุฑ ุงุณูนฺฉ ูพุฑ ูุงุฑุช ุญุงุตู ฺฉุฑ ู  ุฌุงุฆฺบ ุงูุฑ ูุณุชูุจู ุจูุงุฆฺบ!
:::
